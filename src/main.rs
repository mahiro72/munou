use std::collections::HashMap;
use regex::Regex;
use vibrato::{Dictionary, Tokenizer};
use std::fs;
use std::io::{self, Write};
use lazyrand;

// ç‰¹æ®Šãªå˜èªID
static TOP_WORD_ID: isize = 0;
static END_WORD_ID: isize = 1;

// äººå·¥ç„¡è„³ã®æ§‹é€ ä½“
struct MarkovChain {
    words: Vec<String>, // å˜èªã®ãƒªã‚¹ãƒˆ
    word_hash: HashMap<String, isize>, // å˜èªã¨IDã®å¯¾å¿œè¡¨
    chain: HashMap<(isize, isize), Vec<isize>>, // ãƒãƒ«ã‚³ãƒ•é€£é–ã®è¾æ›¸
    tokenizer: Tokenizer, // å½¢æ…‹ç´ è§£æå™¨
}

impl MarkovChain {
    pub fn new() -> Self {
        // å½¢æ…‹ç´ è§£æã®è¾æ›¸ã‚’èª­ã¿è¾¼ã‚€
        let reader = zstd::Decoder::new(fs::File::open("system.dic.zst").unwrap()).unwrap();
        let dict = Dictionary::read(reader).unwrap();

        // å˜èªè¾æ›¸ã®åˆæœŸåŒ–
        let mut word_hash = HashMap::new();
        word_hash.insert("â˜…".to_string(), TOP_WORD_ID);
        word_hash.insert("ã€‚".to_string(), END_WORD_ID);

        let mut words = Vec::new();
        words.push("â˜…".to_string());
        words.push("ã€‚".to_string());
        MarkovChain{
            words,
            word_hash,
            chain: HashMap::new(),
            tokenizer: Tokenizer::new(dict),
        }
    }

    // å˜èªã‚’IDã‚’å–å¾—ã™ã‚‹ã€‚ãªã‘ã‚Œã°æ–°ãŸã«IDã‚’å‰²ã‚Šå½“ã¦ã‚‹
    pub fn get_word_id(&mut self, word: &str) -> isize {
        if let Some(&id) = self.word_hash.get(word) {
            return id;
        }
        let id = self.words.len() as isize;
        self.word_hash.insert(word.to_string(), id);
        self.words.push(word.to_string());
        id
    }

    // æ–‡ç« ã‚’å½¢æ…‹ç´ è§£æã§åˆ†å‰²ã™ã‚‹
    fn split(&self, text: &str) -> Vec<String> {
        let mut worker = self.tokenizer.new_worker();
        worker.reset_sentence(text); // ä»¥å‰ã®è§£æçµæœã‚’ç ´æ£„ã—ã€æ–°ã—ã„æ–‡ç« ã‚’ã‚»ãƒƒãƒˆ
        worker.tokenize();
        worker.token_iter().map(|t| t.surface().to_string()).collect()
    }

    // ãƒãƒ«ã‚³ãƒ•é€£é–ã®è¾æ›¸ã‚’ä½œæˆã™ã‚‹
    pub fn train(&mut self, text: &str) {
        // æ­£è¦è¡¨ç¾ã§ä¸è¦ãªæ–‡å­—ã®å‰Šé™¤
        let re = Regex::new(r"(ã€Š.*?ã€‹|ï¼».*?ï¼½|[ï½œ\s\u{3000}\-]|[ã€Œã€ã€ã€])").unwrap();
        let clean_text = re.replace_all(text, "");
        // å½¢æ…‹ç´ è§£æ
        let words: Vec<String> = self.split(&clean_text);
        // å˜èªIDã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        let word_ids: Vec<isize> = words.iter().map(|w| self.get_word_id(w)).collect();
        // ãƒãƒ«ã‚³ãƒ•é€£é–ã®è¾æ›¸ã‚’ä½œæˆ
        let mut tmp = vec![TOP_WORD_ID, TOP_WORD_ID];
        for word_id in word_ids {
            tmp.push(word_id);
            if tmp.len() < 3 { continue; }
            if tmp.len() > 4 {tmp.remove(0);}
            let w = tmp[2];
            let key = (tmp[0], tmp[1]);
            if self.chain.contains_key(&key) {
                self.chain.get_mut(&key).unwrap().push(w);
            } else {
                self.chain.insert(key, vec![w]);
            }
            if w == END_WORD_ID {
                tmp.clear();
                tmp.extend([TOP_WORD_ID, TOP_WORD_ID]);
            }
        }
    }

    // æ–‡ç« ã‚’ç”Ÿæˆã™ã‚‹
    pub fn generate(&mut self) -> String {
        self.generate_text(TOP_WORD_ID, TOP_WORD_ID)
    }

    // æ¬¡ã®å˜èªIDã‚’ç”Ÿæˆã™ã‚‹
    fn generate_next_id(&self, w1: isize, w2: isize) -> isize {
        let w_ids = match self.chain.get(&(w1, w2)) {
            Some(w_ids) => w_ids,
            None => return END_WORD_ID,
        };
        if w_ids.is_empty() { return END_WORD_ID; }
        lazyrand::choice(w_ids).unwrap()
    }

    // w1, w2ã«ç¶šãæ–‡ç« ã‚’ç”Ÿæˆã™ã‚‹
    pub fn generate_text(&self, w1: isize, w2: isize) -> String {
        let mut result = String::new();
        let mut w1 = w1;
        let mut w2 = w2;
        result.push_str(self.words[w1 as usize].as_str());
        result.push_str(self.words[w2 as usize].as_str());
        let mut w3;
        loop {
            w3 = self.generate_next_id(w1, w2);
            result.push_str(self.words[w3 as usize].as_str());
            if w3 == END_WORD_ID {
                break;
            }
            w1 = w2;
            w2 = w3;
        }
        result.replace("â˜…", "")
    }
}

fn main() {
    // äººå·¥ç„¡èƒ½ã‚’åˆæœŸåŒ–
    let mut markov = MarkovChain::new();
    // ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§å­¦ç¿’
    let text = fs::read_to_string("wagahaiwa_nekodearu.txt").unwrap();
    let lines: Vec<&str> = text.split("\n").collect();
    for line in lines {
        markov.train(line);
    }

    // å¯¾è©±ãƒ¢ãƒ¼ãƒ‰
    println!(">>> çµ‚äº†ã™ã‚‹ã«ã¯Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚\n>>> è©±ã—ã‹ã‘ã¦ã¿ã¾ã—ã‚‡ã†ã€‚");
    loop {
        io::stdout().flush().unwrap();
        let mut input = String::new();
        print!(">>> ");
        io::stdout().flush().unwrap();
        io::stdin().read_line(&mut input).unwrap();
        if input.trim().is_empty() {
            break;
        }
        // å…¥åŠ›ã—ãŸå†…å®¹ã‹ã‚‰é©å½“ã«ååˆºã‚’æŠ½å‡ºã—ã¦ã€ãã‚Œã‚’å«ã‚€æ–‡ç« ã®ç”Ÿæˆ
        let mut worker = markov.tokenizer.new_worker();
        worker.reset_sentence(input);
        worker.tokenize();
        let words: Vec<String> = worker.token_iter()
            .filter(|t| t.feature().contains("åè©"))
            .map(|t| t.surface().to_string())
            .collect();
        let output = if words.len() == 0 {
            markov.generate()
        } else {
            let word: String = lazyrand::choice(&words).unwrap();
            let word_id = markov.get_word_id(&word);
            let text = markov.generate_text(TOP_WORD_ID, word_id);
            if text.len() < word.len() + 4 {
                markov.generate()
            } else {
                text
            }
        };
        println!("æˆ‘è¼©ğŸ±> {}", output);
    }
}
